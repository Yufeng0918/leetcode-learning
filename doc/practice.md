# 实践

## Redis集合

### 列表（list）

#### 压缩列表

当列表中存储的数据量比较小的时候，列表就可以采用压缩列表的方式实现。具体需要同时满足下面两个条件

+ 列表中保存的单个数据（有可能是字符串类型的）小于 64 字节
+ 列表中数据个数少于 512 个

![](../images/leetcode-109.jpg)

压缩列表这种存储结构，一方面比较节省内存，另一方面可以支持不同类型数据的存储。

#### 双向链表

```C
typedef struct listnode {
  struct listNode *prev;
  struct listNode *next;
  void *value;
} listNode;


typedef struct list {
  listNode *head;
  listNode *tail;
  unsigned long len;
  // ....省略其他定义
} list;
```



### 字典（hash）

#### 压缩列表

只有当存储的数据量比较小的情况下，Redis 才使用压缩列表来实现字典类型。具体需要满足两个条件

+ 字典中保存的键和值的大小都要小于 64 字节
+ 字典中键值对的个数要小于 512 个。

#### 散列表

当数据动态增加之后，散列表的装载因子会不停地变大。为了避免散列表性能的下降，当装载因子大于 1 的时候，Redis 会触发扩容，将散列表扩大为原来大小的 2 倍左右

当数据动态减少之后，为了节省内存，当装载因子小于 0.1 的时候，Redis 就会触发缩容，缩小为字典中数据个数的大约 2 倍大小



### 集合（set）

#### 有序数组

Redis 就采用有序数组，来实现集合这种数据类型

+ 存储的数据都是整数
+ 存储的数据元素个数不超过 512 个。

#### 散列表



### 有序集合（sortedset）

#### 有序数组

Redis 就采用有序数组，来实现集合这种数据类型

+ 所有数据的大小都要小于 64 字节
+ 元素个数要小于 128 个

#### 跳表

存储一组数据，并且每个数据会附带一个得分。通过得分的大小，我们将数据组织成跳表这样的数据结构，以支持快速地按照得分值、得分区间获取数据。



### 持久化

#### 只存储数据

**第一种是清除原有的存储结构，只将数据存储到磁盘中。**当我们需要从磁盘还原数据到内存的时候，再重新将数据组织成原来的数据结构。实际上，Redis 采用的就是这种持久化思路。

这种方式也有一定的弊端。**那就是数据从硬盘还原到内存的过程，会耗用比较多的时间。**



#### 存储数据和结构

保留原来的存储格式，将数据按照原有的格式存储在磁盘中。我们拿散列表这样的数据结构来举例。我们可以将散列表的大小、每个数据被散列到的槽的编号等信息，都保存在磁盘中。有了这些信息，**我们从磁盘中将数据还原到内存中的时候，就可以避免重新计算哈希值。**



## 搜索引擎

### 搜集

#### 待爬取网页链接文件：links.bin

在广度优先搜索爬取页面的过程中，爬虫会不停地解析页面链接，将其放到队列中。

#### 网页判重文件：bloom_filter.bin

使用布隆过滤器，我们就可以快速并且非常节省内存地实现网页的判重。

#### 原始网页存储文件：doc_raw.bin

如果我们把每个网页都存储为一个独立的文件，那磁盘中的文件就会非常多，数量可能会有几千万，甚至上亿。常用的文件系统显然不适合存储如此多的文件。**所以，我们可以把多个网页存储在一个文件中。**

![](../images/leetcode-110.jpg)

#### 网页链接及其编号的对应文件：doc_id.bin

我们维护一个中心的计数器，每爬取到一个网页之后，就从计数器中拿一个号码，分配给这个网页，然后计数器加一。在存储网页的同时，**我们将网页链接跟编号之间的对应关系，存储在另一个 doc_id.bin 文件中**

### 分析

#### 抽取网页文本信息

利用 AC 自动机这种多模式串匹配算法，在网页这个大字符串中，一次性查找<stlye>, <scripti>,<option> 关键字，并向后遍历把标签删除。

#### 分词并创建临时索引

字典也叫词库，里面包含大量常用的词语。我们借助词库并采用最长匹配规则，来对文本进行分词。所谓最长匹配，也就是匹配尽可能长的词语。

每个网页的文本信息在分词完成之后，我们都得到一组单词列表。我们把单词与网页之间的对应关系，写入到一个临时索引文件中（tmp_Index.bin），这个临时索引文件用来构建倒排索引文件。临时索引文件的格式如下：

![](../images/leetcode-111.jpg)

给单词编号的方式，跟给网页编号类似。我们维护一个计数器，每当从网页文本信息中分割出一个新的单词的时候，我们就从计数器中取一个编号，分配给它，然后计数器加一。

当所有的网页处理（分词及写入临时索引）完成之后，我们再将这个单词跟编号之间的对应关系，写入到磁盘文件中，并命名为 **term_id.bin。**

经过分析阶段，我们得到了两个重要的文件。它们分别是**临时索引文件（tmp_index.bin）**和**单词编号文件（term_id.bin）。**

### 索引

索引阶段主要负责将分析阶段产生的临时索引，构建成倒排索引。倒排索引（ Inverted index）中记录了每个单词以及包含它的网页列表。

![](../images/leetcode-112.jpg)

并排序的处理思想，将其分割成多个小文件，先对每个小文件独立排序，最后再合并在一起。当然，实际的软件开发中，我们其实可以直接利用 MapReduce 来处理。

![](../images/leetcode-113.jpg)

除了倒排文件之外，我们还需要一个文件，**来记录每个单词编号在倒排索引文件中的偏移位置。我们把这个文件命名为 term_offset.bin。**

![](../images/leetcode-114.jpg)

### 查询

前面三个阶段的处理，只是为了最后的查询做铺垫。因此，现在我们就要利用之前产生的几个文件，来实现最终的用户搜索功能。

+ doc_id.bin：记录网页链接和编号之间的对应关系。
+ term_id.bin：记录单词和编号之间的对应关系。
+ term_offsert.bin：记录每个单词编号在倒排索引文件中的偏移位置。
+ index.bin：倒排索引文件，记录每个单词编号以及对应包含它的网页编号列表。

查找顺序

+ 我们拿这 k 个单词，去 term_id.bin 对应的散列表中，查找对应的单词编号。经过这个查询之后，我们得到了这 k 个单词对应的单词编号。

+ 我们拿这 k 个单词编号，去 term_offset.bin 对应的散列表中，查找每个单词编号在倒排索引文件中的偏移位置。经过这个查询之后，我们得到了 k 个偏移位置。
+ 我们拿这 k 个偏移位置，去倒排索引（index.bin）中，查找 k 个单词对应的包含它的网页编号列表。经过这一步查询之后，我们得到了 k 个网页编号列表。

